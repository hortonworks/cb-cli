package policies

// This file was generated by the swagger tool.
// Editing this file might prove futile when you re-run the swagger generate command

import (
	"github.com/go-swagger/go-swagger/client"

	strfmt "github.com/go-swagger/go-swagger/strfmt"
)

// New creates a new policies API client.
func New(transport client.Transport, formats strfmt.Registry) *Client {
	return &Client{transport: transport, formats: formats}
}

/*
Client for policies API
*/
type Client struct {
	transport client.Transport
	formats   strfmt.Registry
}

/*
AddScaling creates policy

Scaling is the ability to increase or decrease the capacity of the Hadoop cluster or application based on an alert. When scaling policies are used, the capacity is automatically increased or decreased according to the conditions defined. Cloudbreak will do the heavy lifting and based on the alerts and the scaling policy linked to them it executes the associated policy. We scaling granularity is at the hostgroup level - thus you have the option to scale services or components only, not the whole cluster.
*/
func (a *Client) AddScaling(params *AddScalingParams) (*AddScalingOK, error) {
	// TODO: Validate the params before sending
	if params == nil {
		params = NewAddScalingParams()
	}

	result, err := a.transport.Submit(&client.Operation{
		ID:                 "addScaling",
		Method:             "POST",
		PathPattern:        "/clusters/{clusterId}/policies",
		ProducesMediaTypes: []string{"application/json"},
		ConsumesMediaTypes: []string{"application/json"},
		Schemes:            []string{"http", "https"},
		Params:             params,
		Reader:             &AddScalingReader{formats: a.formats},
	})
	if err != nil {
		return nil, err
	}
	return result.(*AddScalingOK), nil
}

/*
DeletePolicy deletes policy

Scaling is the ability to increase or decrease the capacity of the Hadoop cluster or application based on an alert. When scaling policies are used, the capacity is automatically increased or decreased according to the conditions defined. Cloudbreak will do the heavy lifting and based on the alerts and the scaling policy linked to them it executes the associated policy. We scaling granularity is at the hostgroup level - thus you have the option to scale services or components only, not the whole cluster.
*/
func (a *Client) DeletePolicy(params *DeletePolicyParams) error {
	// TODO: Validate the params before sending
	if params == nil {
		params = NewDeletePolicyParams()
	}

	_, err := a.transport.Submit(&client.Operation{
		ID:                 "deletePolicy",
		Method:             "DELETE",
		PathPattern:        "/clusters/{clusterId}/policies/{policyId}",
		ProducesMediaTypes: []string{"application/json"},
		ConsumesMediaTypes: []string{"application/json"},
		Schemes:            []string{"http", "https"},
		Params:             params,
		Reader:             &DeletePolicyReader{formats: a.formats},
	})
	if err != nil {
		return err
	}
	return nil
}

/*
GetScaling retrieves policy

Scaling is the ability to increase or decrease the capacity of the Hadoop cluster or application based on an alert. When scaling policies are used, the capacity is automatically increased or decreased according to the conditions defined. Cloudbreak will do the heavy lifting and based on the alerts and the scaling policy linked to them it executes the associated policy. We scaling granularity is at the hostgroup level - thus you have the option to scale services or components only, not the whole cluster.
*/
func (a *Client) GetScaling(params *GetScalingParams) (*GetScalingOK, error) {
	// TODO: Validate the params before sending
	if params == nil {
		params = NewGetScalingParams()
	}

	result, err := a.transport.Submit(&client.Operation{
		ID:                 "getScaling",
		Method:             "GET",
		PathPattern:        "/clusters/{clusterId}/policies",
		ProducesMediaTypes: []string{"application/json"},
		ConsumesMediaTypes: []string{"application/json"},
		Schemes:            []string{"http", "https"},
		Params:             params,
		Reader:             &GetScalingReader{formats: a.formats},
	})
	if err != nil {
		return nil, err
	}
	return result.(*GetScalingOK), nil
}

/*
SetScaling modifies policy

Scaling is the ability to increase or decrease the capacity of the Hadoop cluster or application based on an alert. When scaling policies are used, the capacity is automatically increased or decreased according to the conditions defined. Cloudbreak will do the heavy lifting and based on the alerts and the scaling policy linked to them it executes the associated policy. We scaling granularity is at the hostgroup level - thus you have the option to scale services or components only, not the whole cluster.
*/
func (a *Client) SetScaling(params *SetScalingParams) (*SetScalingOK, error) {
	// TODO: Validate the params before sending
	if params == nil {
		params = NewSetScalingParams()
	}

	result, err := a.transport.Submit(&client.Operation{
		ID:                 "setScaling",
		Method:             "PUT",
		PathPattern:        "/clusters/{clusterId}/policies/{policyId}",
		ProducesMediaTypes: []string{"application/json"},
		ConsumesMediaTypes: []string{"application/json"},
		Schemes:            []string{"http", "https"},
		Params:             params,
		Reader:             &SetScalingReader{formats: a.formats},
	})
	if err != nil {
		return nil, err
	}
	return result.(*SetScalingOK), nil
}

// SetTransport changes the transport on the client
func (a *Client) SetTransport(transport client.Transport) {
	a.transport = transport
}
